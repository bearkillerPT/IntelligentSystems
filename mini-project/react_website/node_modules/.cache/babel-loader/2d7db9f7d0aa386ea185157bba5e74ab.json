{"ast":null,"code":"import sentence from './01-sentences/index.js';\nimport term from './02-terms/index.js';\nimport whitespace from './03-whitespace/index.js';\nimport normalize from '../compute/normal/index.js';\nimport killUnicode from './unicode.js'; // turn a string input into a 'document' json format\n\nvar fromString = function fromString(input, world) {\n  var methods = world.methods,\n      model = world.model;\n  var _methods$one$tokenize = methods.one.tokenize,\n      splitSentences = _methods$one$tokenize.splitSentences,\n      splitTerms = _methods$one$tokenize.splitTerms,\n      splitWhitespace = _methods$one$tokenize.splitWhitespace;\n  input = input || ''; // split into sentences\n\n  var sentences = splitSentences(input, model); // split into word objects\n\n  input = sentences.map(function (txt) {\n    var terms = splitTerms(txt, model); // split into [pre-text-post]\n\n    terms = terms.map(splitWhitespace); // add normalized term format, always\n\n    terms.forEach(function (t) {\n      normalize(t, world);\n    });\n    return terms;\n  });\n  return input;\n};\n\nexport default {\n  one: {\n    killUnicode: killUnicode,\n    tokenize: {\n      splitSentences: sentence,\n      splitTerms: term,\n      splitWhitespace: whitespace,\n      fromString: fromString\n    }\n  }\n};","map":null,"metadata":{},"sourceType":"module"}